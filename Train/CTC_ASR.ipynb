{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic Speech Recognition (ASR) with CTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the necessary packages\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the source vocabulary\n",
    "\n",
    "# Default word tokens\n",
    "BLK_token = 0  # Blank label\n",
    "PAD_token = 1  # Used for padding short utterances\n",
    "\n",
    "class ASR_Vocab(object):\n",
    "    def __init__(self, digit_seqs):\n",
    "        super(ASR_Vocab, self).__init__()\n",
    "        self.digit2index = {}\n",
    "        self.digit2count = {}\n",
    "        self.index2digit = {BLK_token: \"BLK\", PAD_token: \"PAD\"}\n",
    "        \n",
    "        # Count SOS, EOS, PAD, EMP\n",
    "        self.num_tokens  = 2\n",
    "        self.digit_seqs  = digit_seqs\n",
    "        \n",
    "    def dig2idx(self, digit):\n",
    "        if digit in self.digit2index:\n",
    "            return self.digit2index[digit]\n",
    "        \n",
    "    def idx2dig(self, idx):\n",
    "        if idx in self.index2digit:\n",
    "            return self.index2digit[idx]\n",
    "            \n",
    "    def add_digit(self, digit):\n",
    "        if digit in self.digit2index:\n",
    "            self.digit2count[digit] += 1\n",
    "            \n",
    "        else:\n",
    "            self.digit2index[digit] = self.num_tokens\n",
    "            self.index2digit[self.num_tokens] = digit\n",
    "            self.digit2count[digit] = 1\n",
    "            self.num_tokens += 1\n",
    "            \n",
    "    def build_vocab(self):        \n",
    "        for seq in self.digit_seqs:\n",
    "            for digit in seq:      # Ignore EOS token\n",
    "                self.add_digit(digit)\n",
    "            \n",
    "        # print(\"Vocabulary created with %d tokens ...\" % self.num_tokens)\n",
    "        # return self.num_tokens\n",
    "    \n",
    "    def vocab_size(self):\n",
    "        return self.num_tokens\n",
    "    \n",
    "    def vocabulary(self):\n",
    "        for idx in self.index2digit:\n",
    "            print(idx, self.index2digit[idx])\n",
    "    \n",
    "    def encode(self, seq):\n",
    "        return [self.dig2idx(digit) for digit in seq]\n",
    "    \n",
    "    def decode(self, seq):\n",
    "        return \"\".join([self.idx2dig(idx) for idx in seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ASR utterances :  8511\n",
      "\n",
      "Original sequence :  17914\n",
      "Encoded sequence  :  [7, 5, 8, 7, 3]\n",
      "Decoded sequence  :  17914\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary of audio files and their corresponding ASR labels\n",
    "audio_df = pd.read_csv('./data/ASR/data.txt', sep=\",\")\n",
    "audio_df = audio_df.drop(['Gender', 'Spk_ID', 'Utt_ID'], axis=1)\n",
    "# print(audio_df.head())\n",
    "\n",
    "# Create the vocabulary    \n",
    "asr_labels = audio_df['Transcription'].values\n",
    "\n",
    "print(\"Number of ASR utterances : \", len(asr_labels))\n",
    "vocab_asr = ASR_Vocab(asr_labels)\n",
    "vocab_asr.build_vocab()\n",
    "\n",
    "# Test the vocabulary\n",
    "print(\"\\nOriginal sequence : \", asr_labels[4])\n",
    "encoded_seq = vocab_asr.encode(asr_labels[4])\n",
    "decoded_seq = vocab_asr.decode(encoded_seq)\n",
    "\n",
    "print(\"Encoded sequence  : \", encoded_seq)\n",
    "print(\"Decoded sequence  : \", decoded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2070 Super with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "### Define the device\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8511, 322, 32])\n"
     ]
    }
   ],
   "source": [
    "# Load the audio files\n",
    "import torchaudio\n",
    "\n",
    "n_mels = 32\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_fft=512, win_length=512, hop_length=256, n_mels=n_mels, f_min=0.0, f_max=None, pad=0, power=2.0, normalized=False)\n",
    "\n",
    "audio_files = audio_df['Audio'].values\n",
    "data_path = './data/ASR/train/'\n",
    "\n",
    "mfccs = []\n",
    "max_feat_len = 0\n",
    "for i, audio_file in enumerate(audio_files):\n",
    "    # Load the audio file\n",
    "    audio_path = data_path + audio_file\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Convert to mono and resample to 16kHz\n",
    "    waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    if sample_rate != 16000 : waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
    "    \n",
    "    # Obtain and normalize the MFCCs\n",
    "    mel_spec = mel_spectrogram(waveform)\n",
    "    mel_spec = abs(mel_spec)\n",
    "    mel_spec = (mel_spec - mel_spec.mean()) / mel_spec.std()\n",
    "    mfccs.append(mel_spec)\n",
    "    max_feat_len = max(max_feat_len, mel_spec.shape[2])\n",
    "    \n",
    "# Pad the MFCCs to the maximum length\n",
    "for i, mfcc in enumerate(mfccs):\n",
    "    mfccs[i] = F.pad(mfcc, (0, max_feat_len - mfcc.shape[2]), \"constant\", 0)\n",
    "    \n",
    "mfccs = torch.cat(mfccs, dim=0)\n",
    "mfccs = mfccs.permute(0, 2, 1)\n",
    "print(mfccs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8511, 7])\n"
     ]
    }
   ],
   "source": [
    "# Create the output labels\n",
    "labels = []\n",
    "max_labl_len = 0\n",
    "for i, label in enumerate(asr_labels):\n",
    "    labels.append(vocab_asr.encode(label))\n",
    "    max_labl_len = max(max_labl_len, len(labels[i]))\n",
    "    \n",
    "# Pad the labels to the maximum length\n",
    "for i, label in enumerate(labels):\n",
    "    labels[i] = label + [PAD_token] * (max_labl_len - len(label))\n",
    "    \n",
    "labels = torch.tensor(labels)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloader\n",
    "\n",
    "class ASR_Dataset(Dataset):\n",
    "    def __init__(self, mfccs, labels):\n",
    "        self.mfccs = mfccs\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.mfccs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.mfccs[idx], self.labels[idx]\n",
    "    \n",
    "dataset = ASR_Dataset(mfccs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the seq2seq model\n",
    "input_size  = n_mels\n",
    "hidden_size = 300\n",
    "output_size = vocab_asr.vocab_size()\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CTC based ASR model\n",
    "\n",
    "class CTC_ASR(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CTC_ASR, self).__init__()\n",
    "        self.input_size = input_size        # U\n",
    "        self.hidden_size = hidden_size      # H\n",
    "        self.output_size = output_size      # V\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv1d(input_size, input_size, 10, 2, 5)\n",
    "        self.maxp1 = nn.MaxPool1d(2, 2)\n",
    "        self.batn1 = nn.BatchNorm1d(input_size)\n",
    "        \n",
    "        # GRU layer\n",
    "        self.U = nn.Linear(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers=2, bidirectional=True, batch_first=True)\n",
    "        self.W = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def _init_hidden(self, batch_size):\n",
    "        return torch.zeros(4, batch_size, self.hidden_size).to(device)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        hidden = self._init_hidden(inputs.shape[0])\n",
    "        \n",
    "        inputs = inputs.transpose(1, 2)                                 # B x U x L\n",
    "        cnn_out = F.gelu(self.batn1(self.conv1(inputs)))                # B x U x L'        \n",
    "        cnn_out = cnn_out.transpose(1, 2)                               # B x L' x U\n",
    "        \n",
    "        gru_in = F.relu(self.U(cnn_out))                                # B x L' x H\n",
    "        outputs, hidden = self.gru(gru_in, hidden)                      # B x L' x 2H, 4 x B x H\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:] # B x L' x H\n",
    "        \n",
    "        outputs = self.W(outputs)                                       # B x L' x V\n",
    "        # print(outputs.shape, hidden.shape)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        outputs = self.forward(inputs)\n",
    "        outputs = F.softmax(outputs, dim=2)\n",
    "        \n",
    "        return torch.argmax(outputs, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of CTC_ASR(\n",
      "  (conv1): Conv1d(32, 32, kernel_size=(10,), stride=(2,), padding=(5,))\n",
      "  (maxp1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (batn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (U): Linear(in_features=32, out_features=300, bias=True)\n",
      "  (gru): GRU(300, 300, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (W): Linear(in_features=300, out_features=13, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "ctc_asr = CTC_ASR(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "print(ctc_asr.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer and loss function\n",
    "learning_rate = 0.001\n",
    "\n",
    "ctc_asr_optimizer = optim.Adam(ctc_asr.parameters(), lr=learning_rate)\n",
    "criterion = nn.CTCLoss(blank=0, zero_infinity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started ...\n",
      "Epoch: 1/100, Step: 1/133, Loss: 0.2228\n",
      "Epoch: 1/100, Step: 2/133, Loss: 0.1175\n",
      "Epoch: 1/100, Step: 3/133, Loss: 0.1588\n",
      "Epoch: 1/100, Step: 4/133, Loss: 0.2672\n",
      "Epoch: 1/100, Step: 5/133, Loss: 0.1646\n",
      "Epoch: 1/100, Step: 6/133, Loss: 0.1789\n",
      "Epoch: 1/100, Step: 7/133, Loss: 0.3478\n",
      "Epoch: 1/100, Step: 8/133, Loss: 0.2332\n",
      "Epoch: 1/100, Step: 9/133, Loss: 0.1251\n",
      "Epoch: 1/100, Step: 10/133, Loss: 0.1056\n",
      "Epoch: 1/100, Step: 11/133, Loss: 0.3321\n",
      "Epoch: 1/100, Step: 12/133, Loss: 0.2779\n",
      "Epoch: 1/100, Step: 13/133, Loss: 0.1892\n",
      "Epoch: 1/100, Step: 14/133, Loss: 0.2318\n",
      "Epoch: 1/100, Step: 15/133, Loss: 0.2783\n",
      "Epoch: 1/100, Step: 16/133, Loss: 0.2489\n",
      "Epoch: 1/100, Step: 17/133, Loss: 0.0533\n",
      "Epoch: 1/100, Step: 18/133, Loss: 0.2193\n",
      "Epoch: 1/100, Step: 19/133, Loss: 0.2725\n",
      "Epoch: 1/100, Step: 20/133, Loss: 0.1055\n",
      "Epoch: 1/100, Step: 21/133, Loss: 0.0934\n",
      "Epoch: 1/100, Step: 22/133, Loss: 0.3188\n",
      "Epoch: 1/100, Step: 23/133, Loss: 0.2413\n",
      "Epoch: 1/100, Step: 24/133, Loss: 0.0782\n",
      "Epoch: 1/100, Step: 25/133, Loss: 0.1889\n",
      "Epoch: 1/100, Step: 26/133, Loss: 0.2704\n",
      "Epoch: 1/100, Step: 27/133, Loss: 0.1404\n",
      "Epoch: 1/100, Step: 28/133, Loss: 0.0239\n",
      "Epoch: 1/100, Step: 29/133, Loss: 0.3153\n",
      "Epoch: 1/100, Step: 30/133, Loss: 0.1747\n",
      "Epoch: 1/100, Step: 31/133, Loss: 0.0929\n",
      "Epoch: 1/100, Step: 32/133, Loss: 0.2043\n",
      "Epoch: 1/100, Step: 33/133, Loss: 0.2521\n",
      "Epoch: 1/100, Step: 34/133, Loss: 0.1474\n",
      "Epoch: 1/100, Step: 35/133, Loss: 0.1266\n",
      "Epoch: 1/100, Step: 36/133, Loss: 0.2082\n",
      "Epoch: 1/100, Step: 37/133, Loss: 0.1852\n",
      "Epoch: 1/100, Step: 38/133, Loss: 0.1166\n",
      "Epoch: 1/100, Step: 39/133, Loss: 0.1333\n",
      "Epoch: 1/100, Step: 40/133, Loss: 0.237\n",
      "Epoch: 1/100, Step: 41/133, Loss: 0.1853\n",
      "Epoch: 1/100, Step: 42/133, Loss: 0.0578\n",
      "Epoch: 1/100, Step: 43/133, Loss: 0.2014\n",
      "Epoch: 1/100, Step: 44/133, Loss: 0.2542\n",
      "Epoch: 1/100, Step: 45/133, Loss: 0.1211\n",
      "Epoch: 1/100, Step: 46/133, Loss: 0.0717\n",
      "Epoch: 1/100, Step: 47/133, Loss: 0.2351\n",
      "Epoch: 1/100, Step: 48/133, Loss: 0.1691\n",
      "Epoch: 1/100, Step: 49/133, Loss: 0.0743\n",
      "Epoch: 1/100, Step: 50/133, Loss: 0.1373\n",
      "Epoch: 1/100, Step: 51/133, Loss: 0.2128\n",
      "Epoch: 1/100, Step: 52/133, Loss: 0.2155\n",
      "Epoch: 1/100, Step: 53/133, Loss: 0.1091\n",
      "Epoch: 1/100, Step: 54/133, Loss: 0.1799\n",
      "Epoch: 1/100, Step: 55/133, Loss: 0.195\n",
      "Epoch: 1/100, Step: 56/133, Loss: 0.1046\n",
      "Epoch: 1/100, Step: 57/133, Loss: 0.0861\n",
      "Epoch: 1/100, Step: 58/133, Loss: 0.2061\n",
      "Epoch: 1/100, Step: 59/133, Loss: 0.1695\n",
      "Epoch: 1/100, Step: 60/133, Loss: 0.14\n",
      "Epoch: 1/100, Step: 61/133, Loss: 0.1558\n",
      "Epoch: 1/100, Step: 62/133, Loss: 0.2034\n",
      "Epoch: 1/100, Step: 63/133, Loss: 0.1216\n",
      "Epoch: 1/100, Step: 64/133, Loss: 0.0928\n",
      "Epoch: 1/100, Step: 65/133, Loss: 0.142\n",
      "Epoch: 1/100, Step: 66/133, Loss: 0.1504\n",
      "Epoch: 1/100, Step: 67/133, Loss: 0.0811\n",
      "Epoch: 1/100, Step: 68/133, Loss: 0.1624\n",
      "Epoch: 1/100, Step: 69/133, Loss: 0.1978\n",
      "Epoch: 1/100, Step: 70/133, Loss: 0.1267\n",
      "Epoch: 1/100, Step: 71/133, Loss: 0.1124\n",
      "Epoch: 1/100, Step: 72/133, Loss: 0.1498\n",
      "Epoch: 1/100, Step: 73/133, Loss: 0.1375\n",
      "Epoch: 1/100, Step: 74/133, Loss: 0.0982\n",
      "Epoch: 1/100, Step: 75/133, Loss: 0.1276\n",
      "Epoch: 1/100, Step: 76/133, Loss: 0.1365\n",
      "Epoch: 1/100, Step: 77/133, Loss: 0.1638\n",
      "Epoch: 1/100, Step: 78/133, Loss: -0.0634\n",
      "Epoch: 1/100, Step: 79/133, Loss: 0.6529\n",
      "Epoch: 1/100, Step: 80/133, Loss: -0.0164\n",
      "Epoch: 1/100, Step: 81/133, Loss: -0.2909\n",
      "Epoch: 1/100, Step: 82/133, Loss: 0.4542\n",
      "Epoch: 1/100, Step: 83/133, Loss: 0.3658\n",
      "Epoch: 1/100, Step: 84/133, Loss: -0.1463\n",
      "Epoch: 1/100, Step: 85/133, Loss: 0.0886\n",
      "Epoch: 1/100, Step: 86/133, Loss: 0.3563\n",
      "Epoch: 1/100, Step: 87/133, Loss: 0.0859\n",
      "Epoch: 1/100, Step: 88/133, Loss: 0.033\n",
      "Epoch: 1/100, Step: 89/133, Loss: 0.2739\n",
      "Epoch: 1/100, Step: 90/133, Loss: 0.1617\n",
      "Epoch: 1/100, Step: 91/133, Loss: -0.032\n",
      "Epoch: 1/100, Step: 92/133, Loss: 0.1455\n",
      "Epoch: 1/100, Step: 93/133, Loss: 0.3323\n",
      "Epoch: 1/100, Step: 94/133, Loss: 0.098\n",
      "Epoch: 1/100, Step: 95/133, Loss: -0.0883\n",
      "Epoch: 1/100, Step: 96/133, Loss: 0.2753\n",
      "Epoch: 1/100, Step: 97/133, Loss: 0.3005\n",
      "Epoch: 1/100, Step: 98/133, Loss: -0.0471\n",
      "Epoch: 1/100, Step: 99/133, Loss: -0.0333\n",
      "Epoch: 1/100, Step: 100/133, Loss: 0.3293\n",
      "Epoch: 1/100, Step: 101/133, Loss: 0.2363\n",
      "Epoch: 1/100, Step: 102/133, Loss: -0.0533\n",
      "Epoch: 1/100, Step: 103/133, Loss: 0.0879\n",
      "Epoch: 1/100, Step: 104/133, Loss: 0.3039\n",
      "Epoch: 1/100, Step: 105/133, Loss: 0.1023\n",
      "Epoch: 1/100, Step: 106/133, Loss: 0.0179\n",
      "Epoch: 1/100, Step: 107/133, Loss: 0.2032\n",
      "Epoch: 1/100, Step: 108/133, Loss: 0.2006\n",
      "Epoch: 1/100, Step: 109/133, Loss: 0.0561\n",
      "Epoch: 1/100, Step: 110/133, Loss: 0.1142\n",
      "Epoch: 1/100, Step: 111/133, Loss: 0.2435\n",
      "Epoch: 1/100, Step: 112/133, Loss: 0.0679\n",
      "Epoch: 1/100, Step: 113/133, Loss: 0.0844\n",
      "Epoch: 1/100, Step: 114/133, Loss: 0.2591\n",
      "Epoch: 1/100, Step: 115/133, Loss: 0.1738\n",
      "Epoch: 1/100, Step: 116/133, Loss: 0.0071\n",
      "Epoch: 1/100, Step: 117/133, Loss: 0.1266\n",
      "Epoch: 1/100, Step: 118/133, Loss: 0.2114\n",
      "Epoch: 1/100, Step: 119/133, Loss: 0.1193\n",
      "Epoch: 1/100, Step: 120/133, Loss: 0.058\n",
      "Epoch: 1/100, Step: 121/133, Loss: 0.1815\n",
      "Epoch: 1/100, Step: 122/133, Loss: 0.2142\n",
      "Epoch: 1/100, Step: 123/133, Loss: 0.0725\n",
      "Epoch: 1/100, Step: 124/133, Loss: 0.0689\n",
      "Epoch: 1/100, Step: 125/133, Loss: 0.1602\n",
      "Epoch: 1/100, Step: 126/133, Loss: 0.1637\n",
      "Epoch: 1/100, Step: 127/133, Loss: 0.0414\n",
      "Epoch: 1/100, Step: 128/133, Loss: 0.12\n",
      "Epoch: 1/100, Step: 129/133, Loss: 0.2477\n",
      "Epoch: 1/100, Step: 130/133, Loss: 0.107\n",
      "Epoch: 1/100, Step: 131/133, Loss: 0.0254\n",
      "Epoch: 1/100, Step: 132/133, Loss: 0.188\n",
      "Epoch: 2/100, Step: 1/133, Loss: 0.1401\n",
      "Epoch: 2/100, Step: 2/133, Loss: 0.0315\n",
      "Epoch: 2/100, Step: 3/133, Loss: 0.1037\n",
      "Epoch: 2/100, Step: 4/133, Loss: 0.1833\n",
      "Epoch: 2/100, Step: 5/133, Loss: 0.1423\n",
      "Epoch: 2/100, Step: 6/133, Loss: 0.1096\n",
      "Epoch: 2/100, Step: 7/133, Loss: 0.1221\n",
      "Epoch: 2/100, Step: 8/133, Loss: 0.1041\n",
      "Epoch: 2/100, Step: 9/133, Loss: 0.1124\n",
      "Epoch: 2/100, Step: 10/133, Loss: 0.1421\n",
      "Epoch: 2/100, Step: 11/133, Loss: 0.0839\n",
      "Epoch: 2/100, Step: 12/133, Loss: 0.0935\n",
      "Epoch: 2/100, Step: 13/133, Loss: 0.1665\n",
      "Epoch: 2/100, Step: 14/133, Loss: 0.1538\n",
      "Epoch: 2/100, Step: 15/133, Loss: 0.0702\n",
      "Epoch: 2/100, Step: 16/133, Loss: 0.1391\n",
      "Epoch: 2/100, Step: 17/133, Loss: 0.1157\n",
      "Epoch: 2/100, Step: 18/133, Loss: 0.1026\n",
      "Epoch: 2/100, Step: 19/133, Loss: 0.0815\n",
      "Epoch: 2/100, Step: 20/133, Loss: 0.1585\n",
      "Epoch: 2/100, Step: 21/133, Loss: 0.1117\n",
      "Epoch: 2/100, Step: 22/133, Loss: 0.084\n",
      "Epoch: 2/100, Step: 23/133, Loss: 0.1726\n",
      "Epoch: 2/100, Step: 24/133, Loss: 0.1539\n",
      "Epoch: 2/100, Step: 25/133, Loss: 0.0626\n",
      "Epoch: 2/100, Step: 26/133, Loss: 0.106\n",
      "Epoch: 2/100, Step: 27/133, Loss: 0.1492\n",
      "Epoch: 2/100, Step: 28/133, Loss: 0.0899\n",
      "Epoch: 2/100, Step: 29/133, Loss: 0.0803\n",
      "Epoch: 2/100, Step: 30/133, Loss: 0.1252\n",
      "Epoch: 2/100, Step: 31/133, Loss: 0.1262\n",
      "Epoch: 2/100, Step: 32/133, Loss: 0.1071\n",
      "Epoch: 2/100, Step: 33/133, Loss: 0.1121\n",
      "Epoch: 2/100, Step: 34/133, Loss: 0.1284\n",
      "Epoch: 2/100, Step: 35/133, Loss: 0.089\n",
      "Epoch: 2/100, Step: 36/133, Loss: 0.1161\n",
      "Epoch: 2/100, Step: 37/133, Loss: 0.1584\n",
      "Epoch: 2/100, Step: 38/133, Loss: 0.0672\n",
      "Epoch: 2/100, Step: 39/133, Loss: 0.1097\n",
      "Epoch: 2/100, Step: 40/133, Loss: 0.1431\n",
      "Epoch: 2/100, Step: 41/133, Loss: 0.1438\n",
      "Epoch: 2/100, Step: 42/133, Loss: 0.0836\n",
      "Epoch: 2/100, Step: 43/133, Loss: 0.1051\n",
      "Epoch: 2/100, Step: 44/133, Loss: 0.1835\n",
      "Epoch: 2/100, Step: 45/133, Loss: 0.1013\n",
      "Epoch: 2/100, Step: 46/133, Loss: 0.0957\n",
      "Epoch: 2/100, Step: 47/133, Loss: 0.1436\n",
      "Epoch: 2/100, Step: 48/133, Loss: 0.1632\n",
      "Epoch: 2/100, Step: 49/133, Loss: 0.0903\n",
      "Epoch: 2/100, Step: 50/133, Loss: 0.0984\n",
      "Epoch: 2/100, Step: 51/133, Loss: 0.1536\n",
      "Epoch: 2/100, Step: 52/133, Loss: 0.0768\n",
      "Epoch: 2/100, Step: 53/133, Loss: 0.0677\n",
      "Epoch: 2/100, Step: 54/133, Loss: 0.1562\n",
      "Epoch: 2/100, Step: 55/133, Loss: 0.1226\n",
      "Epoch: 2/100, Step: 56/133, Loss: 0.0951\n",
      "Epoch: 2/100, Step: 57/133, Loss: 0.1544\n",
      "Epoch: 2/100, Step: 58/133, Loss: 0.1396\n",
      "Epoch: 2/100, Step: 59/133, Loss: 0.085\n",
      "Epoch: 2/100, Step: 60/133, Loss: 0.116\n",
      "Epoch: 2/100, Step: 61/133, Loss: 0.1177\n",
      "Epoch: 2/100, Step: 62/133, Loss: 0.104\n",
      "Epoch: 2/100, Step: 63/133, Loss: 0.1441\n",
      "Epoch: 2/100, Step: 64/133, Loss: 0.1456\n",
      "Epoch: 2/100, Step: 65/133, Loss: 0.0721\n",
      "Epoch: 2/100, Step: 66/133, Loss: 0.1442\n",
      "Epoch: 2/100, Step: 67/133, Loss: 0.1517\n",
      "Epoch: 2/100, Step: 68/133, Loss: 0.0953\n",
      "Epoch: 2/100, Step: 69/133, Loss: 0.0819\n",
      "Epoch: 2/100, Step: 70/133, Loss: 0.138\n",
      "Epoch: 2/100, Step: 71/133, Loss: 0.1427\n",
      "Epoch: 2/100, Step: 72/133, Loss: 0.0606\n",
      "Epoch: 2/100, Step: 73/133, Loss: 0.1047\n",
      "Epoch: 2/100, Step: 74/133, Loss: 0.1615\n",
      "Epoch: 2/100, Step: 75/133, Loss: 0.1249\n",
      "Epoch: 2/100, Step: 76/133, Loss: 0.131\n",
      "Epoch: 2/100, Step: 77/133, Loss: 0.1355\n",
      "Epoch: 2/100, Step: 78/133, Loss: 0.1376\n",
      "Epoch: 2/100, Step: 79/133, Loss: 0.0903\n",
      "Epoch: 2/100, Step: 80/133, Loss: 0.077\n",
      "Epoch: 2/100, Step: 81/133, Loss: 0.1822\n",
      "Epoch: 2/100, Step: 82/133, Loss: 0.1112\n",
      "Epoch: 2/100, Step: 83/133, Loss: 0.1146\n",
      "Epoch: 2/100, Step: 84/133, Loss: 0.1494\n",
      "Epoch: 2/100, Step: 85/133, Loss: 0.1603\n",
      "Epoch: 2/100, Step: 86/133, Loss: 0.0658\n",
      "Epoch: 2/100, Step: 87/133, Loss: 0.1395\n",
      "Epoch: 2/100, Step: 88/133, Loss: 0.1075\n",
      "Epoch: 2/100, Step: 89/133, Loss: 0.1374\n",
      "Epoch: 2/100, Step: 90/133, Loss: 0.0956\n",
      "Epoch: 2/100, Step: 91/133, Loss: 0.1787\n",
      "Epoch: 2/100, Step: 92/133, Loss: 0.1279\n",
      "Epoch: 2/100, Step: 93/133, Loss: 0.0579\n",
      "Epoch: 2/100, Step: 94/133, Loss: 0.1095\n",
      "Epoch: 2/100, Step: 95/133, Loss: 0.1333\n",
      "Epoch: 2/100, Step: 96/133, Loss: 0.1297\n",
      "Epoch: 2/100, Step: 97/133, Loss: 0.159\n",
      "Epoch: 2/100, Step: 98/133, Loss: 0.132\n",
      "Epoch: 2/100, Step: 99/133, Loss: 0.1148\n",
      "Epoch: 2/100, Step: 100/133, Loss: 0.1008\n",
      "Epoch: 2/100, Step: 101/133, Loss: 0.1237\n",
      "Epoch: 2/100, Step: 102/133, Loss: 0.0999\n",
      "Epoch: 2/100, Step: 103/133, Loss: 0.1202\n",
      "Epoch: 2/100, Step: 104/133, Loss: 0.12\n",
      "Epoch: 2/100, Step: 105/133, Loss: 0.1227\n",
      "Epoch: 2/100, Step: 106/133, Loss: 0.1495\n",
      "Epoch: 2/100, Step: 107/133, Loss: 0.1236\n",
      "Epoch: 2/100, Step: 108/133, Loss: 0.104\n",
      "Epoch: 2/100, Step: 109/133, Loss: 0.1099\n",
      "Epoch: 2/100, Step: 110/133, Loss: 0.0936\n",
      "Epoch: 2/100, Step: 111/133, Loss: 0.1106\n",
      "Epoch: 2/100, Step: 112/133, Loss: 0.094\n",
      "Epoch: 2/100, Step: 113/133, Loss: 0.1151\n",
      "Epoch: 2/100, Step: 114/133, Loss: 0.1412\n",
      "Epoch: 2/100, Step: 115/133, Loss: 0.1162\n",
      "Epoch: 2/100, Step: 116/133, Loss: 0.0898\n",
      "Epoch: 2/100, Step: 117/133, Loss: 0.1264\n",
      "Epoch: 2/100, Step: 118/133, Loss: 0.0763\n",
      "Epoch: 2/100, Step: 119/133, Loss: 0.0724\n",
      "Epoch: 2/100, Step: 120/133, Loss: 0.1546\n",
      "Epoch: 2/100, Step: 121/133, Loss: 0.106\n",
      "Epoch: 2/100, Step: 122/133, Loss: 0.1196\n",
      "Epoch: 2/100, Step: 123/133, Loss: 0.1257\n",
      "Epoch: 2/100, Step: 124/133, Loss: 0.1373\n",
      "Epoch: 2/100, Step: 125/133, Loss: 0.0835\n",
      "Epoch: 2/100, Step: 126/133, Loss: 0.0694\n",
      "Epoch: 2/100, Step: 127/133, Loss: 0.1336\n",
      "Epoch: 2/100, Step: 128/133, Loss: 0.0923\n",
      "Epoch: 2/100, Step: 129/133, Loss: 0.0683\n",
      "Epoch: 2/100, Step: 130/133, Loss: 0.1452\n",
      "Epoch: 2/100, Step: 131/133, Loss: 0.106\n",
      "Epoch: 2/100, Step: 132/133, Loss: 0.0871\n",
      "Epoch: 3/100, Step: 1/133, Loss: 0.1261\n",
      "Epoch: 3/100, Step: 2/133, Loss: 0.0926\n",
      "Epoch: 3/100, Step: 3/133, Loss: 0.0815\n",
      "Epoch: 3/100, Step: 4/133, Loss: 0.1649\n",
      "Epoch: 3/100, Step: 5/133, Loss: 0.0855\n",
      "Epoch: 3/100, Step: 6/133, Loss: 0.0745\n",
      "Epoch: 3/100, Step: 7/133, Loss: 0.1512\n",
      "Epoch: 3/100, Step: 8/133, Loss: 0.1198\n",
      "Epoch: 3/100, Step: 9/133, Loss: 0.0692\n",
      "Epoch: 3/100, Step: 10/133, Loss: 0.1239\n",
      "Epoch: 3/100, Step: 11/133, Loss: 0.0866\n",
      "Epoch: 3/100, Step: 12/133, Loss: 0.0581\n",
      "Epoch: 3/100, Step: 13/133, Loss: 0.1269\n",
      "Epoch: 3/100, Step: 14/133, Loss: 0.1592\n",
      "Epoch: 3/100, Step: 15/133, Loss: 0.0846\n",
      "Epoch: 3/100, Step: 16/133, Loss: 0.0628\n",
      "Epoch: 3/100, Step: 17/133, Loss: 0.166\n",
      "Epoch: 3/100, Step: 18/133, Loss: 0.0788\n",
      "Epoch: 3/100, Step: 19/133, Loss: 0.0648\n",
      "Epoch: 3/100, Step: 20/133, Loss: 0.1701\n",
      "Epoch: 3/100, Step: 21/133, Loss: 0.1174\n",
      "Epoch: 3/100, Step: 22/133, Loss: 0.072\n",
      "Epoch: 3/100, Step: 23/133, Loss: 0.0894\n",
      "Epoch: 3/100, Step: 24/133, Loss: 0.1492\n",
      "Epoch: 3/100, Step: 25/133, Loss: 0.07\n",
      "Epoch: 3/100, Step: 26/133, Loss: 0.0772\n",
      "Epoch: 3/100, Step: 27/133, Loss: 0.1815\n",
      "Epoch: 3/100, Step: 28/133, Loss: 0.0762\n",
      "Epoch: 3/100, Step: 29/133, Loss: 0.0409\n",
      "Epoch: 3/100, Step: 30/133, Loss: 0.1899\n",
      "Epoch: 3/100, Step: 31/133, Loss: 0.0843\n",
      "Epoch: 3/100, Step: 32/133, Loss: 0.035\n",
      "Epoch: 3/100, Step: 33/133, Loss: 0.1444\n",
      "Epoch: 3/100, Step: 34/133, Loss: 0.1381\n",
      "Epoch: 3/100, Step: 35/133, Loss: 0.0336\n",
      "Epoch: 3/100, Step: 36/133, Loss: 0.1163\n",
      "Epoch: 3/100, Step: 37/133, Loss: 0.162\n",
      "Epoch: 3/100, Step: 38/133, Loss: 0.0985\n",
      "Epoch: 3/100, Step: 39/133, Loss: 0.0675\n",
      "Epoch: 3/100, Step: 40/133, Loss: 0.1085\n",
      "Epoch: 3/100, Step: 41/133, Loss: 0.1141\n",
      "Epoch: 3/100, Step: 42/133, Loss: 0.0716\n",
      "Epoch: 3/100, Step: 43/133, Loss: 0.0854\n",
      "Epoch: 3/100, Step: 44/133, Loss: 0.1208\n",
      "Epoch: 3/100, Step: 45/133, Loss: 0.0948\n",
      "Epoch: 3/100, Step: 46/133, Loss: 0.1236\n",
      "Epoch: 3/100, Step: 47/133, Loss: 0.0729\n",
      "Epoch: 3/100, Step: 48/133, Loss: 0.1195\n",
      "Epoch: 3/100, Step: 49/133, Loss: 0.1009\n",
      "Epoch: 3/100, Step: 50/133, Loss: 0.1116\n",
      "Epoch: 3/100, Step: 51/133, Loss: 0.0694\n",
      "Epoch: 3/100, Step: 52/133, Loss: 0.1365\n",
      "Epoch: 3/100, Step: 53/133, Loss: 0.1107\n",
      "Epoch: 3/100, Step: 54/133, Loss: 0.0849\n",
      "Epoch: 3/100, Step: 55/133, Loss: 0.1008\n",
      "Epoch: 3/100, Step: 56/133, Loss: 0.1198\n",
      "Epoch: 3/100, Step: 57/133, Loss: 0.1329\n",
      "Epoch: 3/100, Step: 58/133, Loss: 0.0598\n",
      "Epoch: 3/100, Step: 59/133, Loss: 0.1282\n",
      "Epoch: 3/100, Step: 60/133, Loss: 0.1226\n",
      "Epoch: 3/100, Step: 61/133, Loss: 0.0876\n",
      "Epoch: 3/100, Step: 62/133, Loss: 0.0894\n",
      "Epoch: 3/100, Step: 63/133, Loss: 0.1065\n",
      "Epoch: 3/100, Step: 64/133, Loss: 0.1072\n",
      "Epoch: 3/100, Step: 65/133, Loss: 0.0934\n",
      "Epoch: 3/100, Step: 66/133, Loss: 0.1504\n",
      "Epoch: 3/100, Step: 67/133, Loss: 0.1389\n",
      "Epoch: 3/100, Step: 68/133, Loss: 0.1279\n",
      "Epoch: 3/100, Step: 69/133, Loss: 0.0998\n",
      "Epoch: 3/100, Step: 70/133, Loss: 0.1292\n",
      "Epoch: 3/100, Step: 71/133, Loss: 0.0688\n",
      "Epoch: 3/100, Step: 72/133, Loss: 0.1835\n",
      "Epoch: 3/100, Step: 73/133, Loss: 0.2047\n",
      "Epoch: 3/100, Step: 74/133, Loss: 0.092\n",
      "Epoch: 3/100, Step: 75/133, Loss: 0.1377\n",
      "Epoch: 3/100, Step: 76/133, Loss: 0.1617\n",
      "Epoch: 3/100, Step: 77/133, Loss: 0.0689\n",
      "Epoch: 3/100, Step: 78/133, Loss: 0.0703\n",
      "Epoch: 3/100, Step: 79/133, Loss: 0.212\n",
      "Epoch: 3/100, Step: 80/133, Loss: 0.0882\n",
      "Epoch: 3/100, Step: 81/133, Loss: 0.0787\n",
      "Epoch: 3/100, Step: 82/133, Loss: 0.179\n",
      "Epoch: 3/100, Step: 83/133, Loss: 0.213\n",
      "Epoch: 3/100, Step: 84/133, Loss: 0.0448\n",
      "Epoch: 3/100, Step: 85/133, Loss: 0.1505\n",
      "Epoch: 3/100, Step: 86/133, Loss: 0.1809\n",
      "Epoch: 3/100, Step: 87/133, Loss: 0.0383\n",
      "Epoch: 3/100, Step: 88/133, Loss: 0.143\n",
      "Epoch: 3/100, Step: 89/133, Loss: 0.1668\n",
      "Epoch: 3/100, Step: 90/133, Loss: 0.1016\n",
      "Epoch: 3/100, Step: 91/133, Loss: 0.1174\n",
      "Epoch: 3/100, Step: 92/133, Loss: 0.1988\n",
      "Epoch: 3/100, Step: 93/133, Loss: 0.0961\n",
      "Epoch: 3/100, Step: 94/133, Loss: 0.0842\n",
      "Epoch: 3/100, Step: 95/133, Loss: 0.1491\n",
      "Epoch: 3/100, Step: 96/133, Loss: 0.1026\n",
      "Epoch: 3/100, Step: 97/133, Loss: 0.105\n",
      "Epoch: 3/100, Step: 98/133, Loss: 0.1429\n",
      "Epoch: 3/100, Step: 99/133, Loss: 0.1012\n",
      "Epoch: 3/100, Step: 100/133, Loss: 0.1106\n",
      "Epoch: 3/100, Step: 101/133, Loss: 0.1557\n",
      "Epoch: 3/100, Step: 102/133, Loss: 0.0792\n",
      "Epoch: 3/100, Step: 103/133, Loss: 0.1559\n",
      "Epoch: 3/100, Step: 104/133, Loss: 0.0787\n",
      "Epoch: 3/100, Step: 105/133, Loss: 0.0913\n",
      "Epoch: 3/100, Step: 106/133, Loss: 0.1488\n",
      "Epoch: 3/100, Step: 107/133, Loss: 0.1377\n",
      "Epoch: 3/100, Step: 108/133, Loss: 0.1046\n",
      "Epoch: 3/100, Step: 109/133, Loss: 0.1471\n",
      "Epoch: 3/100, Step: 110/133, Loss: 0.1431\n",
      "Epoch: 3/100, Step: 111/133, Loss: 0.1176\n",
      "Epoch: 3/100, Step: 112/133, Loss: 0.0611\n",
      "Epoch: 3/100, Step: 113/133, Loss: 0.0961\n",
      "Epoch: 3/100, Step: 114/133, Loss: 0.1273\n",
      "Epoch: 3/100, Step: 115/133, Loss: 0.1222\n",
      "Epoch: 3/100, Step: 116/133, Loss: 0.1189\n",
      "Epoch: 3/100, Step: 117/133, Loss: 0.1449\n",
      "Epoch: 3/100, Step: 118/133, Loss: 0.1337\n",
      "Epoch: 3/100, Step: 119/133, Loss: 0.0545\n",
      "Epoch: 3/100, Step: 120/133, Loss: 0.125\n",
      "Epoch: 3/100, Step: 121/133, Loss: 0.0878\n",
      "Epoch: 3/100, Step: 122/133, Loss: 0.0761\n",
      "Epoch: 3/100, Step: 123/133, Loss: 0.0857\n",
      "Epoch: 3/100, Step: 124/133, Loss: 0.1576\n",
      "Epoch: 3/100, Step: 125/133, Loss: 0.1341\n",
      "Epoch: 3/100, Step: 126/133, Loss: 0.1099\n",
      "Epoch: 3/100, Step: 127/133, Loss: 0.1001\n",
      "Epoch: 3/100, Step: 128/133, Loss: 0.1284\n",
      "Epoch: 3/100, Step: 129/133, Loss: 0.0677\n",
      "Epoch: 3/100, Step: 130/133, Loss: 0.0726\n",
      "Epoch: 3/100, Step: 131/133, Loss: 0.1895\n",
      "Epoch: 3/100, Step: 132/133, Loss: 0.1394\n",
      "Epoch: 4/100, Step: 1/133, Loss: 0.0606\n",
      "Epoch: 4/100, Step: 2/133, Loss: 0.1833\n",
      "Epoch: 4/100, Step: 3/133, Loss: 0.1603\n",
      "Epoch: 4/100, Step: 4/133, Loss: -0.0078\n",
      "Epoch: 4/100, Step: 5/133, Loss: 0.0673\n",
      "Epoch: 4/100, Step: 6/133, Loss: 0.209\n",
      "Epoch: 4/100, Step: 7/133, Loss: 0.0176\n",
      "Epoch: 4/100, Step: 8/133, Loss: 0.0689\n",
      "Epoch: 4/100, Step: 9/133, Loss: 0.1958\n",
      "Epoch: 4/100, Step: 10/133, Loss: 0.1245\n",
      "Epoch: 4/100, Step: 11/133, Loss: 0.0437\n",
      "Epoch: 4/100, Step: 12/133, Loss: 0.1629\n",
      "Epoch: 4/100, Step: 13/133, Loss: 0.1192\n",
      "Epoch: 4/100, Step: 14/133, Loss: 0.0531\n",
      "Epoch: 4/100, Step: 15/133, Loss: 0.1144\n",
      "Epoch: 4/100, Step: 16/133, Loss: 0.1626\n",
      "Epoch: 4/100, Step: 17/133, Loss: 0.0283\n",
      "Epoch: 4/100, Step: 18/133, Loss: 0.133\n",
      "Epoch: 4/100, Step: 19/133, Loss: 0.1636\n",
      "Epoch: 4/100, Step: 20/133, Loss: 0.1181\n",
      "Epoch: 4/100, Step: 21/133, Loss: 0.0568\n",
      "Epoch: 4/100, Step: 22/133, Loss: 0.1947\n",
      "Epoch: 4/100, Step: 23/133, Loss: 0.0787\n",
      "Epoch: 4/100, Step: 24/133, Loss: 0.0831\n",
      "Epoch: 4/100, Step: 25/133, Loss: 0.1248\n",
      "Epoch: 4/100, Step: 26/133, Loss: 0.1274\n",
      "Epoch: 4/100, Step: 27/133, Loss: 0.0917\n",
      "Epoch: 4/100, Step: 28/133, Loss: 0.1081\n",
      "Epoch: 4/100, Step: 29/133, Loss: 0.1413\n",
      "Epoch: 4/100, Step: 30/133, Loss: 0.1142\n",
      "Epoch: 4/100, Step: 31/133, Loss: 0.0659\n",
      "Epoch: 4/100, Step: 32/133, Loss: 0.1137\n",
      "Epoch: 4/100, Step: 33/133, Loss: 0.1135\n",
      "Epoch: 4/100, Step: 34/133, Loss: 0.0779\n",
      "Epoch: 4/100, Step: 35/133, Loss: 0.127\n",
      "Epoch: 4/100, Step: 36/133, Loss: 0.1117\n",
      "Epoch: 4/100, Step: 37/133, Loss: 0.0391\n",
      "Epoch: 4/100, Step: 38/133, Loss: 0.1103\n",
      "Epoch: 4/100, Step: 39/133, Loss: 0.1164\n",
      "Epoch: 4/100, Step: 40/133, Loss: 0.0581\n",
      "Epoch: 4/100, Step: 41/133, Loss: 0.1176\n",
      "Epoch: 4/100, Step: 42/133, Loss: 0.1423\n",
      "Epoch: 4/100, Step: 43/133, Loss: 0.0654\n",
      "Epoch: 4/100, Step: 44/133, Loss: 0.1117\n",
      "Epoch: 4/100, Step: 45/133, Loss: 0.0999\n",
      "Epoch: 4/100, Step: 46/133, Loss: 0.1123\n",
      "Epoch: 4/100, Step: 47/133, Loss: 0.041\n",
      "Epoch: 4/100, Step: 48/133, Loss: 0.0844\n",
      "Epoch: 4/100, Step: 49/133, Loss: 0.159\n",
      "Epoch: 4/100, Step: 50/133, Loss: 0.0266\n",
      "Epoch: 4/100, Step: 51/133, Loss: 0.152\n",
      "Epoch: 4/100, Step: 52/133, Loss: 0.1388\n",
      "Epoch: 4/100, Step: 53/133, Loss: 0.0851\n",
      "Epoch: 4/100, Step: 54/133, Loss: 0.1131\n",
      "Epoch: 4/100, Step: 55/133, Loss: 0.1337\n",
      "Epoch: 4/100, Step: 56/133, Loss: 0.1152\n",
      "Epoch: 4/100, Step: 57/133, Loss: 0.1079\n",
      "Epoch: 4/100, Step: 58/133, Loss: 0.1036\n",
      "Epoch: 4/100, Step: 59/133, Loss: 0.0685\n",
      "Epoch: 4/100, Step: 60/133, Loss: 0.1073\n",
      "Epoch: 4/100, Step: 61/133, Loss: 0.1054\n",
      "Epoch: 4/100, Step: 62/133, Loss: 0.0859\n",
      "Epoch: 4/100, Step: 63/133, Loss: 0.154\n",
      "Epoch: 4/100, Step: 64/133, Loss: 0.1445\n",
      "Epoch: 4/100, Step: 65/133, Loss: 0.0537\n",
      "Epoch: 4/100, Step: 66/133, Loss: 0.1211\n",
      "Epoch: 4/100, Step: 67/133, Loss: 0.0803\n",
      "Epoch: 4/100, Step: 68/133, Loss: 0.072\n",
      "Epoch: 4/100, Step: 69/133, Loss: 0.1054\n",
      "Epoch: 4/100, Step: 70/133, Loss: 0.1299\n",
      "Epoch: 4/100, Step: 71/133, Loss: 0.1004\n",
      "Epoch: 4/100, Step: 72/133, Loss: 0.1225\n",
      "Epoch: 4/100, Step: 73/133, Loss: 0.0573\n",
      "Epoch: 4/100, Step: 74/133, Loss: 0.1205\n",
      "Epoch: 4/100, Step: 75/133, Loss: 0.0746\n",
      "Epoch: 4/100, Step: 76/133, Loss: 0.1366\n",
      "Epoch: 4/100, Step: 77/133, Loss: 0.0864\n",
      "Epoch: 4/100, Step: 78/133, Loss: 0.0864\n",
      "Epoch: 4/100, Step: 79/133, Loss: 0.1262\n",
      "Epoch: 4/100, Step: 80/133, Loss: 0.1167\n",
      "Epoch: 4/100, Step: 81/133, Loss: 0.0756\n",
      "Epoch: 4/100, Step: 82/133, Loss: 0.1344\n",
      "Epoch: 4/100, Step: 83/133, Loss: 0.1631\n",
      "Epoch: 4/100, Step: 84/133, Loss: 0.0699\n",
      "Epoch: 4/100, Step: 85/133, Loss: 0.0947\n",
      "Epoch: 4/100, Step: 86/133, Loss: 0.1198\n",
      "Epoch: 4/100, Step: 87/133, Loss: 0.0792\n",
      "Epoch: 4/100, Step: 88/133, Loss: 0.1145\n",
      "Epoch: 4/100, Step: 89/133, Loss: 0.0907\n",
      "Epoch: 4/100, Step: 90/133, Loss: 0.0745\n",
      "Epoch: 4/100, Step: 91/133, Loss: 0.1529\n",
      "Epoch: 4/100, Step: 92/133, Loss: 0.1108\n",
      "Epoch: 4/100, Step: 93/133, Loss: 0.0714\n",
      "Epoch: 4/100, Step: 94/133, Loss: 0.0929\n",
      "Epoch: 4/100, Step: 95/133, Loss: 0.0866\n",
      "Epoch: 4/100, Step: 96/133, Loss: 0.1132\n",
      "Epoch: 4/100, Step: 97/133, Loss: 0.05\n",
      "Epoch: 4/100, Step: 98/133, Loss: 0.1174\n",
      "Epoch: 4/100, Step: 99/133, Loss: 0.0855\n",
      "Epoch: 4/100, Step: 100/133, Loss: 0.0923\n",
      "Epoch: 4/100, Step: 101/133, Loss: 0.1769\n",
      "Epoch: 4/100, Step: 102/133, Loss: 0.0807\n",
      "Epoch: 4/100, Step: 103/133, Loss: 0.0732\n",
      "Epoch: 4/100, Step: 104/133, Loss: 0.1399\n",
      "Epoch: 4/100, Step: 105/133, Loss: 0.0506\n",
      "Epoch: 4/100, Step: 106/133, Loss: 0.0638\n",
      "Epoch: 4/100, Step: 107/133, Loss: 0.1304\n",
      "Epoch: 4/100, Step: 108/133, Loss: 0.1053\n",
      "Epoch: 4/100, Step: 109/133, Loss: 0.0856\n",
      "Epoch: 4/100, Step: 110/133, Loss: 0.1607\n",
      "Epoch: 4/100, Step: 111/133, Loss: 0.0591\n",
      "Epoch: 4/100, Step: 112/133, Loss: 0.0788\n",
      "Epoch: 4/100, Step: 113/133, Loss: 0.1466\n",
      "Epoch: 4/100, Step: 114/133, Loss: 0.0405\n",
      "Epoch: 4/100, Step: 115/133, Loss: 0.0586\n",
      "Epoch: 4/100, Step: 116/133, Loss: 0.1488\n",
      "Epoch: 4/100, Step: 117/133, Loss: 0.0693\n",
      "Epoch: 4/100, Step: 118/133, Loss: 0.0927\n",
      "Epoch: 4/100, Step: 119/133, Loss: 0.1219\n",
      "Epoch: 4/100, Step: 120/133, Loss: 0.095\n",
      "Epoch: 4/100, Step: 121/133, Loss: 0.0509\n",
      "Epoch: 4/100, Step: 122/133, Loss: 0.119\n",
      "Epoch: 4/100, Step: 123/133, Loss: 0.0702\n",
      "Epoch: 4/100, Step: 124/133, Loss: 0.0638\n",
      "Epoch: 4/100, Step: 125/133, Loss: 0.087\n",
      "Epoch: 4/100, Step: 126/133, Loss: 0.0892\n",
      "Epoch: 4/100, Step: 127/133, Loss: 0.0909\n",
      "Epoch: 4/100, Step: 128/133, Loss: 0.0765\n",
      "Epoch: 4/100, Step: 129/133, Loss: 0.1146\n",
      "Epoch: 4/100, Step: 130/133, Loss: 0.0996\n",
      "Epoch: 4/100, Step: 131/133, Loss: 0.0439\n",
      "Epoch: 4/100, Step: 132/133, Loss: 0.0954\n",
      "Epoch: 5/100, Step: 1/133, Loss: 0.1103\n",
      "Epoch: 5/100, Step: 2/133, Loss: 0.0426\n",
      "Epoch: 5/100, Step: 3/133, Loss: 0.1165\n",
      "Epoch: 5/100, Step: 4/133, Loss: 0.1109\n",
      "Epoch: 5/100, Step: 5/133, Loss: 0.0816\n",
      "Epoch: 5/100, Step: 6/133, Loss: 0.0978\n",
      "Epoch: 5/100, Step: 7/133, Loss: 0.1125\n",
      "Epoch: 5/100, Step: 8/133, Loss: 0.0681\n",
      "Epoch: 5/100, Step: 9/133, Loss: 0.0988\n",
      "Epoch: 5/100, Step: 10/133, Loss: 0.1109\n",
      "Epoch: 5/100, Step: 11/133, Loss: 0.0822\n",
      "Epoch: 5/100, Step: 12/133, Loss: 0.1097\n",
      "Epoch: 5/100, Step: 13/133, Loss: 0.1281\n",
      "Epoch: 5/100, Step: 14/133, Loss: 0.0019\n",
      "Epoch: 5/100, Step: 15/133, Loss: 0.1104\n",
      "Epoch: 5/100, Step: 16/133, Loss: 0.1005\n",
      "Epoch: 5/100, Step: 17/133, Loss: 0.0449\n",
      "Epoch: 5/100, Step: 18/133, Loss: 0.0952\n",
      "Epoch: 5/100, Step: 19/133, Loss: 0.1485\n",
      "Epoch: 5/100, Step: 20/133, Loss: 0.0846\n",
      "Epoch: 5/100, Step: 21/133, Loss: 0.1089\n",
      "Epoch: 5/100, Step: 22/133, Loss: 0.1355\n",
      "Epoch: 5/100, Step: 23/133, Loss: 0.0004\n",
      "Epoch: 5/100, Step: 24/133, Loss: 0.1\n",
      "Epoch: 5/100, Step: 25/133, Loss: 0.1601\n",
      "Epoch: 5/100, Step: 26/133, Loss: 0.0616\n",
      "Epoch: 5/100, Step: 27/133, Loss: 0.1093\n",
      "Epoch: 5/100, Step: 28/133, Loss: 0.139\n",
      "Epoch: 5/100, Step: 29/133, Loss: 0.0434\n",
      "Epoch: 5/100, Step: 30/133, Loss: 0.0771\n",
      "Epoch: 5/100, Step: 31/133, Loss: 0.1281\n",
      "Epoch: 5/100, Step: 32/133, Loss: 0.0314\n",
      "Epoch: 5/100, Step: 33/133, Loss: 0.0921\n",
      "Epoch: 5/100, Step: 34/133, Loss: 0.1266\n",
      "Epoch: 5/100, Step: 35/133, Loss: 0.0771\n",
      "Epoch: 5/100, Step: 36/133, Loss: 0.0804\n",
      "Epoch: 5/100, Step: 37/133, Loss: 0.094\n",
      "Epoch: 5/100, Step: 38/133, Loss: 0.1194\n",
      "Epoch: 5/100, Step: 39/133, Loss: 0.0294\n",
      "Epoch: 5/100, Step: 40/133, Loss: 0.109\n",
      "Epoch: 5/100, Step: 41/133, Loss: 0.162\n",
      "Epoch: 5/100, Step: 42/133, Loss: 0.0427\n",
      "Epoch: 5/100, Step: 43/133, Loss: 0.1314\n",
      "Epoch: 5/100, Step: 44/133, Loss: 0.0947\n",
      "Epoch: 5/100, Step: 45/133, Loss: 0.042\n",
      "Epoch: 5/100, Step: 46/133, Loss: 0.1493\n",
      "Epoch: 5/100, Step: 47/133, Loss: 0.1031\n",
      "Epoch: 5/100, Step: 48/133, Loss: 0.0371\n",
      "Epoch: 5/100, Step: 49/133, Loss: 0.2017\n",
      "Epoch: 5/100, Step: 50/133, Loss: 0.0748\n",
      "Epoch: 5/100, Step: 51/133, Loss: 0.0187\n",
      "Epoch: 5/100, Step: 52/133, Loss: 0.1528\n",
      "Epoch: 5/100, Step: 53/133, Loss: 0.077\n",
      "Epoch: 5/100, Step: 54/133, Loss: 0.0058\n",
      "Epoch: 5/100, Step: 55/133, Loss: 0.2062\n",
      "Epoch: 5/100, Step: 56/133, Loss: 0.0791\n",
      "Epoch: 5/100, Step: 57/133, Loss: 0.0536\n",
      "Epoch: 5/100, Step: 58/133, Loss: 0.1293\n",
      "Epoch: 5/100, Step: 59/133, Loss: 0.0765\n",
      "Epoch: 5/100, Step: 60/133, Loss: 0.0128\n",
      "Epoch: 5/100, Step: 61/133, Loss: 0.1762\n",
      "Epoch: 5/100, Step: 62/133, Loss: 0.1374\n",
      "Epoch: 5/100, Step: 63/133, Loss: 0.0015\n",
      "Epoch: 5/100, Step: 64/133, Loss: 0.1321\n",
      "Epoch: 5/100, Step: 65/133, Loss: 0.0925\n",
      "Epoch: 5/100, Step: 66/133, Loss: 0.0477\n",
      "Epoch: 5/100, Step: 67/133, Loss: 0.1141\n",
      "Epoch: 5/100, Step: 68/133, Loss: 0.0782\n",
      "Epoch: 5/100, Step: 69/133, Loss: 0.0609\n",
      "Epoch: 5/100, Step: 70/133, Loss: 0.1649\n",
      "Epoch: 5/100, Step: 71/133, Loss: 0.0144\n",
      "Epoch: 5/100, Step: 72/133, Loss: 0.0737\n",
      "Epoch: 5/100, Step: 73/133, Loss: 0.1497\n",
      "Epoch: 5/100, Step: 74/133, Loss: 0.0173\n",
      "Epoch: 5/100, Step: 75/133, Loss: 0.0305\n",
      "Epoch: 5/100, Step: 76/133, Loss: 0.1508\n",
      "Epoch: 5/100, Step: 77/133, Loss: 0.0571\n",
      "Epoch: 5/100, Step: 78/133, Loss: 0.0494\n",
      "Epoch: 5/100, Step: 79/133, Loss: 0.1189\n",
      "Epoch: 5/100, Step: 80/133, Loss: 0.0613\n",
      "Epoch: 5/100, Step: 81/133, Loss: 0.0683\n",
      "Epoch: 5/100, Step: 82/133, Loss: 0.1025\n",
      "Epoch: 5/100, Step: 83/133, Loss: 0.0478\n",
      "Epoch: 5/100, Step: 84/133, Loss: 0.0933\n",
      "Epoch: 5/100, Step: 85/133, Loss: 0.1049\n",
      "Epoch: 5/100, Step: 86/133, Loss: 0.0405\n",
      "Epoch: 5/100, Step: 87/133, Loss: 0.1075\n",
      "Epoch: 5/100, Step: 88/133, Loss: 0.1081\n",
      "Epoch: 5/100, Step: 89/133, Loss: 0.0345\n",
      "Epoch: 5/100, Step: 90/133, Loss: 0.0864\n",
      "Epoch: 5/100, Step: 91/133, Loss: 0.1191\n",
      "Epoch: 5/100, Step: 92/133, Loss: 0.1237\n",
      "Epoch: 5/100, Step: 93/133, Loss: 0.0856\n",
      "Epoch: 5/100, Step: 94/133, Loss: 0.0864\n",
      "Epoch: 5/100, Step: 95/133, Loss: 0.0378\n",
      "Epoch: 5/100, Step: 96/133, Loss: 0.1022\n",
      "Epoch: 5/100, Step: 97/133, Loss: 0.1226\n",
      "Epoch: 5/100, Step: 98/133, Loss: 0.0395\n",
      "Epoch: 5/100, Step: 99/133, Loss: 0.1198\n",
      "Epoch: 5/100, Step: 100/133, Loss: 0.1575\n",
      "Epoch: 5/100, Step: 101/133, Loss: 0.0436\n",
      "Epoch: 5/100, Step: 102/133, Loss: 0.0873\n",
      "Epoch: 5/100, Step: 103/133, Loss: 0.0993\n",
      "Epoch: 5/100, Step: 104/133, Loss: 0.0757\n",
      "Epoch: 5/100, Step: 105/133, Loss: 0.0903\n",
      "Epoch: 5/100, Step: 106/133, Loss: 0.0948\n",
      "Epoch: 5/100, Step: 107/133, Loss: 0.0789\n",
      "Epoch: 5/100, Step: 108/133, Loss: 0.1056\n",
      "Epoch: 5/100, Step: 109/133, Loss: 0.0421\n",
      "Epoch: 5/100, Step: 110/133, Loss: 0.0709\n",
      "Epoch: 5/100, Step: 111/133, Loss: 0.103\n",
      "Epoch: 5/100, Step: 112/133, Loss: 0.0545\n",
      "Epoch: 5/100, Step: 113/133, Loss: 0.1173\n",
      "Epoch: 5/100, Step: 114/133, Loss: 0.0762\n",
      "Epoch: 5/100, Step: 115/133, Loss: 0.1048\n",
      "Epoch: 5/100, Step: 116/133, Loss: 0.1298\n",
      "Epoch: 5/100, Step: 117/133, Loss: 0.0323\n",
      "Epoch: 5/100, Step: 118/133, Loss: 0.0449\n",
      "Epoch: 5/100, Step: 119/133, Loss: 0.1526\n",
      "Epoch: 5/100, Step: 120/133, Loss: 0.0283\n",
      "Epoch: 5/100, Step: 121/133, Loss: 0.0666\n",
      "Epoch: 5/100, Step: 122/133, Loss: 0.1155\n",
      "Epoch: 5/100, Step: 123/133, Loss: 0.0475\n",
      "Epoch: 5/100, Step: 124/133, Loss: 0.1601\n",
      "Epoch: 5/100, Step: 125/133, Loss: 0.0619\n",
      "Epoch: 5/100, Step: 126/133, Loss: 0.027\n",
      "Epoch: 5/100, Step: 127/133, Loss: 0.1556\n",
      "Epoch: 5/100, Step: 128/133, Loss: 0.0336\n",
      "Epoch: 5/100, Step: 129/133, Loss: 0.0213\n",
      "Epoch: 5/100, Step: 130/133, Loss: 0.1675\n",
      "Epoch: 5/100, Step: 131/133, Loss: 0.0622\n",
      "Epoch: 5/100, Step: 132/133, Loss: 0.0925\n",
      "Training completed !!!\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "ctc_asr.train()\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"Training started ...\")\n",
    "for epoch in range(n_epochs):\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        if inputs.size(0) != batch_size:\n",
    "            continue\n",
    "        \n",
    "        # Zero the gradients\n",
    "        ctc_asr.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = ctc_asr(inputs)\n",
    "        \n",
    "        # Compute the loss        \n",
    "        loss = criterion(outputs.transpose(0, 1), labels, torch.tensor([inputs.shape[0]] * batch_size), torch.tensor([labels.shape[1]] * batch_size))\n",
    "        \n",
    "        loss.backward()\n",
    "        ctc_asr_optimizer.step()\n",
    "        print(\"Epoch: {}/{}, Step: {}/{}, Loss: {}\".format(epoch+1, n_epochs, i+1, len(dataloader), np.round(loss.item(), 4)))\n",
    "print(\"Training completed !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "def ctc_decode(output, vocab_asr):\n",
    "    decoded_output = []\n",
    "    prev = -1\n",
    "    for i in range(len(output)):\n",
    "        if output[i] != prev:\n",
    "            decoded_output.append(output[i]) \n",
    "        prev = output[i]\n",
    "    \n",
    "    decoded_output = [x for x in decoded_output if x != 0 and x != 1]\n",
    "    return vocab_asr.decode(decoded_output)\n",
    "\n",
    "def inference(model, waveform, vocab_asr, max_feat_len=322):\n",
    "    # Obtain and normalize the MFCCs\n",
    "    mel_spec = mel_spectrogram(waveform)\n",
    "    mel_spec = abs(mel_spec)\n",
    "    mel_spec = (mel_spec - mel_spec.mean()) / mel_spec.std()\n",
    "    \n",
    "    mel_spec.unsqueeze_(0)\n",
    "    # if mel_spec.shape[2] > max_feat_len:\n",
    "    #     mel_spec = mel_spec[:, :, :max_feat_len]\n",
    "    \n",
    "    # Pad the MFCCs to the maximum length\n",
    "    # else:\n",
    "    #     mel_spec = F.pad(mel_spec, (0, max_feat_len - mel_spec.shape[2]), \"constant\", 0)\n",
    "\n",
    "    mel_spec = mel_spec.transpose(1, 2)\n",
    "    mel_spec = mel_spec.to(device)\n",
    "    \n",
    "    # Predict the output\n",
    "    model.eval()\n",
    "    output = model.predict(mel_spec).squeeze(0)\n",
    "    output = output.tolist()\n",
    "    \n",
    "    return ctc_decode(output, vocab_asr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual transcription : 1Z88153\n",
      "Predicted transcription : 68122Z7\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "ctc_asr.eval()\n",
    "waveform, sample_rate = torchaudio.load(\"./test/68122Z7A.wav\")\n",
    "waveform = waveform.squeeze(0)\n",
    "\n",
    "output_seq = inference(ctc_asr, waveform, vocab_asr)\n",
    "print(\"Actual transcription : \"+ \"1Z88153\")\n",
    "print(\"Predicted transcription : \"+ output_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(ctc_asr.state_dict(), \"./versions/CTC_ASR/ctc_asr_4_100.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4948faae73994ad3d6a7a0e1abe58007870dedbb48704155a4f3fc2daa213808"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
